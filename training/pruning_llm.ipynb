{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_id = \"shenzhi-wang/Llama3-8B-Chinese-Chat\"\n",
    "local_model_path = \"./local_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft\n",
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    init_lora_weights=False\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_1\")\n",
    "model.set_adapter(\"adapter_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Give me a random number.\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
    "        return sample\n",
    "    else:\n",
    "        sample[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}] + sample[\"messages\"]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    "\n",
    "\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove.remove(\"messages\")\n",
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove, batched=False)\n",
    "\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "\n",
    "print(dataset['train'])\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the dataset\n",
    "# def preprocess_function(examples):\n",
    "#     inputs = []\n",
    "#     for messages in examples[\"messages\"]:\n",
    "#         input_text = \"\"\n",
    "#         for message in messages:\n",
    "#             if message[\"role\"] == \"user\":\n",
    "#                 input_text += \"User: \" + message[\"content\"] + \"\\n\"\n",
    "#             elif message[\"role\"] == \"assistant\":\n",
    "#                 input_text += \"Assistant: \" + message[\"content\"] + \"\\n\"\n",
    "#             elif message[\"role\"] == \"system\":\n",
    "#                 input_text += \"System: \" + message[\"content\"] + \"\\n\"\n",
    "#         inputs.append(input_text)\n",
    "#     model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=512)\n",
    "#     return model_inputs\n",
    "\n",
    "# # Preprocess\n",
    "# tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"messages\"])\n",
    "\n",
    "# # Split the dataset\n",
    "# train_dataset = tokenized_datasets[\"train\"]\n",
    "# eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for messages in examples[\"messages\"]:\n",
    "        input_text = \"\"\n",
    "        output_text = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                input_text += \"User: \" + message[\"content\"] + \"\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                if output_text == \"\":\n",
    "                    output_text = \"Assistant: \" + message[\"content\"] + \"\\n\"\n",
    "                else:\n",
    "                    output_text += \"Assistant: \" + message[\"content\"] + \"\\n\"\n",
    "            elif message[\"role\"] == \"system\":\n",
    "                input_text += \"System: \" + message[\"content\"] + \"\\n\"\n",
    "        \n",
    "        inputs.append(input_text)\n",
    "        labels.append(output_text)\n",
    "\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=256)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        model_labels = tokenizer(labels, truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "    model_inputs[\"labels\"] = model_labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# \n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"messages\"])\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(eval_dataset)\n",
    "print(len(train_dataset[0]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,  # Small Batch Size\n",
    "    per_device_eval_batch_size=1,   # Small Batch Size\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import prune\n",
    "\n",
    "\n",
    "# Define Prune Function\n",
    "def prune_model(model, amount=0.5):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')\n",
    "\n",
    "prune_model(model, amount=0.9)  # Pruning 90%\n",
    "\n",
    "\n",
    "def calculate_sparsity(model):\n",
    "    total_weights = 0\n",
    "    total_zero_weights = 0\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            total_weights += module.weight.nelement()\n",
    "            total_zero_weights += torch.sum(module.weight == 0).item()\n",
    "    \n",
    "    sparsity = total_zero_weights / total_weights\n",
    "    print(f\"Model Sparsity: {sparsity:.2%}\")\n",
    "    return sparsity\n",
    "\n",
    "# calculate sparsity and print\n",
    "sparsity = calculate_sparsity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,  \n",
    "    per_device_eval_batch_size=1,  \n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  \n",
    ")\n",
    "\n",
    "\n",
    "# Custom Trainer Class\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, model, inputs):\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "        loss.backward()\n",
    "\n",
    "        # sparse_gradients\n",
    "        # self.sparse_gradients(model)\n",
    "\n",
    "        # allreduce grad\n",
    "        self.allreduce_gradients(model)\n",
    "\n",
    "        # update parameter\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # sparsity = calculate_sparsity(model)\n",
    "        return loss.detach()\n",
    "    \n",
    "    def sparse_gradients(self, model):\n",
    "        sparsity = calculate_sparsity(model)\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                self._calculate_sparsity(param.grad)\n",
    "\n",
    "\n",
    "    def allreduce_gradients(self, model):\n",
    "        for param in model.parameters():\n",
    "            # print(\"need all reduce param.grad\")\n",
    "            pass\n",
    "\n",
    "    def _calculate_sparsity(self, grad):\n",
    "        non_zero = torch.count_nonzero(grad).item()\n",
    "        total_elements = grad.numel()\n",
    "        sparsity = 1 - (non_zero / total_elements)\n",
    "        print(f\"Sparsity: {sparsity:.4f}\")\n",
    "\n",
    "        \n",
    "\n",
    "# Use our  CustomTrainer to train\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_size = total_params * 4  # assuming 32-bit floats (4 bytes per float)\n",
    "    return total_size\n",
    "\n",
    "original_size = calculate_model_size(model)\n",
    "print(f\"Original Model Size (GB): {original_size / (1000*1000)}, Pruned Model Size (GB): {(original_size * (1-sparsity)) / (1000 * 1000)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
